{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython import get_ipython\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datathon/dataset/product_data.csv\")\n",
    "outfit_data = pd.read_csv(\"../datathon/dataset/outfit_data.csv\")\n",
    "print(df.head(), outfit_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new feature that is a list showcasing what outfits does each product appear in\n",
    "outfits_agrupados = outfit_data.groupby('cod_modelo_color')['cod_outfit'].apply(list).reset_index()\n",
    "outfits_agrupados.head()\n",
    "df = pd.merge(df, outfits_agrupados, on='cod_modelo_color', how='left')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"cod_color_code\",\"des_color_specification_esp\",\"des_agrup_color_eng\",\"des_sex\",\"des_age\",\"des_line\",\"des_fabric\",\"des_product_category\",\"des_product_aggregated_family\",\"des_product_family\",\"des_product_type\"]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos cuantos valores unicos hay en cada columna\n",
    "for x in categorical_columns:\n",
    "    print(x, ' = ', df[x].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que existen un cojon de categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plotnumber = 1\n",
    "\n",
    "for column in df:\n",
    "    if plotnumber <= 12 and isinstance(df[column].dtype, pd.CategoricalDtype):\n",
    "        ax = plt.subplot(3, 4, plotnumber)\n",
    "        sns.countplot(x=df[column])\n",
    "        plt.xlabel(column)\n",
    "        plt.xticks(rotation=45)\n",
    "        plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize ResNet50 (pre-trained on ImageNet, remove the final classification layer)\n",
    "# Load the pre-trained ResNet model with locally saved weights\n",
    "resnet_model = ResNet50(weights='../model/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, pooling='avg')\n",
    "\n",
    "# Step 2: Define function to process each image\n",
    "def process_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(239, 334))  # ResNet50 expects 224x224 input\n",
    "    img_data = image.img_to_array(img)                      # Convert to array\n",
    "    img_data = np.expand_dims(img_data, axis=0)             # Expand dimensions to match model input\n",
    "    img_data = preprocess_input(img_data)                   # Preprocess (scale, normalize, etc.)\n",
    "    return img_data\n",
    "\n",
    "# Step 3: Define function to extract image embeddings\n",
    "def extract_image_embedding(img_path):\n",
    "    img_data = process_image(img_path)\n",
    "    embedding = resnet_model.predict(img_data)  # Pass image through the pre-trained ResNet\n",
    "    return embedding.flatten()                  # Flatten the output to get the feature vector\n",
    "\n",
    "# Step 4: Load image paths and extract embeddings\n",
    "image_embeddings = []\n",
    "\n",
    "# Iterate through all image filenames in the dataset\n",
    "i= 0\n",
    "if not os.path.isfile('df_with_embeddings.csv'):\n",
    "    for img_filename in df['des_filename']:\n",
    "        print ('image number ', i)\n",
    "        img_path = os.path.join(os.pardir, img_filename)\n",
    "        embedding = extract_image_embedding(img_path)\n",
    "        image_embeddings.append(embedding)\n",
    "        i += 1\n",
    "\n",
    "# Convert list of embeddings into a NumPy array\n",
    "    df['image_embedding'] = list(image_embeddings)\n",
    "    df.to_csv('df_with_embeddings.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_with_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "print(df[categorical_columns].shape)\n",
    "categorical_columns_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "df['image_embedding'] = df['image_embedding'].apply(lambda x: np.fromstring(x.strip('[]'), sep=' '))\n",
    "image_embeddings = np.vstack(df['image_embedding'].values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embeddings_normalized = scaler.fit_transform(image_embeddings)\n",
    "print(\"Categorical columns encoded shape:\", categorical_columns_encoded.shape)\n",
    "print(\"Embeddings normalized shape:\", embeddings_normalized.shape)\n",
    "\n",
    "# Combine embeddings with encoded categorical features\n",
    "combined_features = np.hstack((embeddings_normalized, categorical_columns_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Definir el modelo DBSCAN\n",
    "dbscan_model = DBSCAN(eps=1.5, min_samples=2, max_samples=1000)\n",
    "dbscan_labels = dbscan_model.fit_predict(combined_features)\n",
    "\n",
    "n_clusters = len(np.unique(dbscan_labels))\n",
    "n_clusters -= 1\n",
    "print(len(dbscan_labels))\n",
    "print(f'Número de clusters: {n_clusters}')\n",
    "if n_clusters >= 1:\n",
    "    # Calcular el Silhouette Score\n",
    "    silhouette_dbscan = silhouette_score(combined_features, dbscan_labels)\n",
    "    print(f'Silhouette Score for DBSCAN: {silhouette_dbscan}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Definir el modelo de clustering jerárquico\n",
    "hierarchical_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5, linkage='ward')\n",
    "hierarchical_labels = hierarchical_model.fit_predict(combined_features)\n",
    "\n",
    "# Calcular el Silhouette Score\n",
    "silhouette_hierarchical = silhouette_score(combined_features, hierarchical_labels)\n",
    "print(f'Numero de clusters: {len(np.unique(hierarchical_labels))}')\n",
    "print(f'Silhouette Score for Hierarchical Clustering: {silhouette_hierarchical}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def cluster_purity(labels):\n",
    "    total_samples = len(labels)\n",
    "    cluster_purity = 0\n",
    "    \n",
    "    for label in set(labels):\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        outfit_counts = Counter(df.iloc[cluster_indices]['cod_outfit'].apply(lambda x: tuple(eval(x))))\n",
    "        most_common_outfit_count = outfit_counts.most_common(1)[0][1]\n",
    "        cluster_purity += most_common_outfit_count\n",
    "    \n",
    "    return cluster_purity / total_samples\n",
    "\n",
    "# Evaluate purity for DBSCAN and Hierarchical clusters\n",
    "purity_dbscan = cluster_purity(dbscan_labels)\n",
    "purity_hierarchical = cluster_purity(hierarchical_labels)\n",
    "\n",
    "print(f'Purity Score for DBSCAN: {purity_dbscan}')\n",
    "print(f'Purity Score for Hierarchical Clustering: {purity_hierarchical}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una mejor un score de pureza mayor en hierarchical clustering, por lo tanto, vamos a hacer un código para la combinaciómn de los tres parámetros del HC para ver que valores nos pueden propocionar los mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "distance_thresholds = [0.5, 1.0, 1.5, 2.0]\n",
    "linkages = ['ward', 'complete', 'average', 'single']\n",
    "n_clusters_list = [5, 10, 15, 20]\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all combinations of the three parameters\n",
    "for distance_threshold in distance_thresholds:\n",
    "    for linkage in linkages:\n",
    "        for n_clusters in n_clusters_list:\n",
    "            try:\n",
    "                # Apply Agglomerative Clustering\n",
    "                hierarchical_model = AgglomerativeClustering(\n",
    "                    n_clusters=None, distance_threshold=distance_threshold, linkage=linkage\n",
    "                )\n",
    "                labels = hierarchical_model.fit_predict(combined_features)\n",
    "                \n",
    "                # Calculate the Silhouette Score\n",
    "                silhouette_hierarchical = silhouette_score(combined_features, hierarchical_labels)\n",
    "                \n",
    "                # Store the result (parameters and Silhouette score)\n",
    "                results.append({\n",
    "                    'distance_threshold': distance_threshold,\n",
    "                    'linkage': linkage,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'silhouette_score': silhouette_hierarchical\n",
    "                })\n",
    "            except Exception as e:\n",
    "                # Store any exceptions that occur during the process\n",
    "                results.append({\n",
    "                    'distance_threshold': distance_threshold,\n",
    "                    'linkage': linkage,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'silhouette_score': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_ids = df[df['cod_modelo_color'].duplicated()]\n",
    "\n",
    "# Mostrar los valores duplicados\n",
    "print(duplicated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results_df['error'][0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Assuming `combined_features` contains the features used for clustering\n",
    "# and `hierarchical_labels` contains the labels from the hierarchical clustering\n",
    "\n",
    "# Perform hierarchical clustering to get the linkage matrix\n",
    "# Z = linkage(combined_features, method='ward')\n",
    "\n",
    "# Store the information of the row at index 6469\n",
    "new_data_point = df.iloc[6469].copy()\n",
    "\n",
    "# Drop the row at index 6469\n",
    "#df = df.drop(6469)\n",
    "\n",
    "def predict_cluster(new_data_point, Z, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Predict the cluster for a new data point based on hierarchical clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data_point: The new data point to be clustered.\n",
    "    - Z: The linkage matrix from hierarchical clustering.\n",
    "    - threshold: The distance threshold for forming clusters.\n",
    "    \n",
    "    Returns:\n",
    "    - The cluster label for the new data point.\n",
    "    \"\"\"\n",
    "    # Combine the new data point with the existing data\n",
    "    combined_data = np.vstack([combined_features, new_data_point])\n",
    "    \n",
    "    # Perform hierarchical clustering on the combined data\n",
    "    #new_labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "    new_labels = hierarchical_model.fit_predict(combined_data)\n",
    "    \n",
    "    # The label for the new data point will be the last label in the array\n",
    "    return new_labels[-1]\n",
    "\n",
    "# Example usage:\n",
    "new_data_point = combined_features[0]  # Replace with actual new data point\n",
    "predicted_cluster = predict_cluster(new_data_point, Z)\n",
    "print(f'The predicted cluster for the new data point is: {predicted_cluster}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to get items in the cluster with label XXXX\n",
    "items_in_cluster = df[hierarchical_labels == predicted_cluster]\n",
    "\n",
    "# Display the filtered items\n",
    "print(\"Printing cod_modelo_color of each item in the cluster\")\n",
    "\n",
    "print(items_in_cluster[\"cod_modelo_color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_outputs(archivo_salida, caracter_inicial):\n",
    "    # Obtener el historial del entorno de ejecución\n",
    "    ipython_historial = get_ipython().history_manager\n",
    "    outputs = get_ipython().user_ns['Out']\n",
    "    \n",
    "    # Abrir el archivo donde se guardarán los outputs\n",
    "    with open(archivo_salida, 'w') as f:\n",
    "        for cell_number, output in outputs.items():\n",
    "            # Verificar si el número de celda empieza con el caracter especificado\n",
    "            if str(cell_number).startswith(caracter_inicial):\n",
    "                f.write(f\"Output de la celda [{cell_number}]:\\n\")\n",
    "                f.write(str(output))\n",
    "                f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            \n",
    "    print(f\"Outputs guardados en {archivo_salida}\")\n",
    "\n",
    "# Llamada a la función\n",
    "guardar_outputs(\"outputs_notebook.txt\",\"# save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
